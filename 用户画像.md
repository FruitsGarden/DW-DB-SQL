# ODS层

## 创建订单详情表
```

		hive (gmall)>
		drop table if exists gmall.ods_order_detail;
		create external table gmall.ods_order_detail( 
 		`id` string COMMENT '订单编号',
 		`order_id` string  COMMENT '订单号', 
		`user_id` string COMMENT '用户id',
 		`sku_id` string COMMENT '商品id',
 		`sku_name` string COMMENT '商品名称',
 		`order_price` string COMMENT '商品价格',
 		`sku_num` string COMMENT '商品数量',
 		`create_time` string COMMENT '创建时间'
		) COMMENT '订单明细表'
		PARTITIONED BY (`dt` string)
		row format delimited fields terminated by '\t' 
		ocation '/warehouse/gmall/ods/ods_order_detail/'
		;
```



## 创建用户表
```
		hive (gmall)>
		drop table if exists gmall.ods_user_info;
		create external table gmall.ods_user_info( 
 		`id` string COMMENT '用户id',
 		`name` string COMMENT '姓名',
 		`birthday` string COMMENT '生日',
 		`gender` string COMMENT '性别',
 		`email` string COMMENT '邮箱',
 		`user_level` string COMMENT '用户等级',
 		`create_time` string COMMENT '创建时间'
		) COMMENT '用户信息'
		PARTITIONED BY (`dt` string)
		row format delimited fields terminated by '\t'
		location '/warehouse/gmall/ods/ods_user_info/'
		;
```

## 创建商品表
```
		hive (gmall)>
		drop table if exists gmall.ods_sku_info;
		create external table gmall.ods_sku_info( 
		`id` string COMMENT 'skuId',
 		`spu_id` string COMMENT 'spuid', 
 		`price` decimal(10,2) COMMENT '价格',
 		`sku_name` string COMMENT '商品名称',
 		`sku_desc` string COMMENT '商品描述',
 		`weight` string COMMENT '重量',
 		`tm_id` string COMMENT '品牌id',
 		`category3_id` string COMMENT '品类id',
 		`create_time` string COMMENT '创建时间'
		) COMMENT '商品表'
		PARTITIONED BY (`dt` string)
		row format delimited fields terminated by '\t'
		location '/warehouse/gmall/ods/ods_sku_info/'
		;
```
## 创建商品一级分类表
```
		hive (gmall)>
		drop table if exists gmall.ods_base_category1;
		create external table gmall.ods_base_category1( 
 		`id` string COMMENT 'id',
 		`name` string COMMENT '名称'
		) COMMENT '商品一级分类'
		PARTITIONED BY (`dt` string)
		row format delimited fields terminated by '\t'
		location '/warehouse/gmall/ods/ods_base_category1/'
		;
```

## 创建商品二级分类表
```
		hive (gmall)>
		drop table if exists gmall.ods_base_category2;
		create external table gmall.ods_base_category2( 
		`id` string COMMENT ' id',
		`name` string COMMENT '名称',
		category1_id string COMMENT '一级品类id'
		) COMMENT '商品二级分类'
		PARTITIONED BY (`dt` string)
		row format delimited fields terminated by '\t'
		location '/warehouse/gmall/ods/ods_base_category2/'
		;
```
## 创建商品三级分类表
```
		hive (gmall)>
		drop table if exists gmall.ods_base_category3;
		create external table gmall.ods_base_category3(
		`id` string COMMENT ' id',
 		`name`  string COMMENT '名称',
 		category2_id string COMMENT '二级品类id'
		) COMMENT '商品三级分类'
		PARTITIONED BY (`dt` string)
		row format delimited fields terminated by '\t'
		location '/warehouse/gmall/ods/ods_base_category3/'
		;
```
##  ODS层数据导入脚本 
### （1）在/home/hadoop/bin目录下创建脚本ods_db.sh
		[hadoop@node1 bin]$ vim ods_db.sh
脚本中填写如下内容
	
	#!/bin/bash
	APP=gmall
   	hive=/opt/bigdata/hive/bin/hive

	# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
	if [ -n "$1" ] ;then
		do_date=$1
	else 
		do_date=`date -d "-1 day" +%F`
	fi

	sql=" 

	load data inpath '/origin_data/$APP/db/order_detail/$do_date' OVERWRITE into table "$APP".ods_order_detail partition(dt='$do_date');

	load data inpath '/origin_data/$APP/db/sku_info/$do_date' OVERWRITE into table "$APP".ods_sku_info partition(dt='$do_date');

	load data inpath '/origin_data/$APP/db/user_info/$do_date' OVERWRITE into table "$APP".ods_user_info partition(dt='$do_date');
	
	load data inpath '/origin_data/$APP/db/base_category1/$do_date' OVERWRITE into table "$APP".ods_base_category1 partition(dt='$do_date');

	load data inpath '/origin_data/$APP/db/base_category2/$do_date' OVERWRITE into table "$APP".ods_base_category2 partition(dt='$do_date');

	load data inpath '/origin_data/$APP/db/base_category3/$do_date' OVERWRITE into table "$APP".ods_base_category3 partition(dt='$do_date'); 
	"
	$hive -e "$sql"

### （2）增加脚本执行权限 ###
		[hadoop@node1 bin]$ chmod 777 ods_db.sh
###  （3）执行脚本导入数据###
		[hadoop@node1 bin]$ ./ods_db.sh 2020-04-28
###  （4）执行脚本导入数据查询导入数据###
		hive (gmall)> 
		select * from gmall.ods_order_info where dt='2020-04-28' limit 1;

# DWD层
对ODS层数据进行判空过滤。对商品分类表进行维度退化（降维）。
## 创建订单详情表 ##
		hive (gmall)>
		drop table if exists gmall.dwd_order_detail;
		create external table gmall.dwd_order_detail( 
		`id` string COMMENT '',
		`order_id` decimal(10,2) COMMENT '', 
		`user_id` string COMMENT 'id',
 		`sku_id` string COMMENT 'id',
 		`sku_name` string COMMENT '',
 		`order_price` string COMMENT '',
 		`sku_num` string COMMENT '',
 		`create_time` string COMMENT ''
		)
		PARTITIONED BY (`dt` string) 
		stored as parquet
		location '/warehouse/gmall/dwd/dwd_order_detail/'
		tblproperties ("parquet.compression"="snappy")
		;

## 创建用户表 ##
		hive (gmall)>
		drop table if exists gmall.dwd_user_info;
		create external table gmall.dwd_user_info( 
 		`id` string COMMENT 'id',
 		`name` string COMMENT '', 
 		`birthday` string COMMENT '',
 		`gender` string COMMENT '',
 		`email` string COMMENT '',
 		`user_level` string COMMENT '',
 		`create_time` string COMMENT ''
		) 
		PARTITIONED BY (`dt` string)
		stored as parquet
		location '/warehouse/gmall/dwd/dwd_user_info/'
		tblproperties ("parquet.compression"="snappy")
		;
## 创建商品表 ##
		hive (gmall)>
		drop table if exists gmall.dwd_sku_info;
		create external table gmall.dwd_sku_info(
		 `id` string COMMENT 'skuId',
		 `spu_id` string COMMENT 'spuid',
		 `price` decimal(10,2) COMMENT '',
		 `sku_name` string COMMENT '',
		 `sku_desc` string COMMENT '',
		 `weight` string COMMENT '',
 		 `tm_id` string COMMENT 'id',
 		 `category3_id` string COMMENT '1id',
		 `category2_id` string COMMENT '2id',
		 `category1_id` string COMMENT '3id',
		 `category3_name` string COMMENT '3',
		 `category2_name` string COMMENT '2',
		 `category1_name` string COMMENT '1',
		 `create_time` string COMMENT ''
		)	 
		PARTITIONED BY (`dt` string)
		stored as parquet
		location '/warehouse/gmall/dwd/dwd_sku_info/'
		tblproperties ("parquet.compression"="snappy")
		;

## DWD层数据导入脚本 ##

### （1）在/home/hadoop/bin目录下创建脚本dwd_db.sh ###
		[hadoop@node1 bin]$ vim dwd_db.sh
在脚本中填写如下内容

		#!/bin/bash

		# 定义变量方便修改
		APP=gmall
		hive=/opt/bigdata/hive/bin/hive

		# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
		if [ -n "$1" ] ;then
			do_date=$1
		else 
			do_date=`date -d "-1 day" +%F`  
		fi 

		sql="

		set hive.exec.dynamic.partition.mode=nonstrict;
		set hive.exec.mode.local.auto=true;
	
		insert overwrite table "$APP".dwd_order_detail partition(dt)
		select * from "$APP".ods_order_detail 
		where dt='$do_date'   and id is not null;
		
		insert overwrite table "$APP".dwd_user_info partition(dt)
		select * from "$APP".ods_user_info
		where dt='$do_date' and id is not null;
	
		insert overwrite table "$APP".dwd_sku_info partition(dt)
		select  
		    sku.id,
		    sku.spu_id,
		    sku.price,
		    sku.sku_name,
		    sku.sku_desc,
		    sku.weight,
		    sku.tm_id,
		    sku.category3_id,
		    c2.id category2_id,
		    c1.id category1_id,
		    c3.name category3_name,
		    c2.name category2_name,
		    c1.name category1_name,
		    sku.create_time,
		    sku.dt
		from
		    "$APP".ods_sku_info sku
			join "$APP".ods_base_category3 c3 on sku.category3_id=c3.id 
		    join "$APP".ods_base_category2 c2 on c3.category2_id=c2.id 
		    join "$APP".ods_base_category1 c1 on c2.category1_id=c1.id 
		where sku.dt='$do_date'  and c2.dt='$do_date'
		and c3.dt='$do_date' and c1.dt='$do_date'
		and sku.id is not null;
		"

		$hive -e "$sql"

### （2）增加脚本执行权限 ###
		[hadoop@node1 bin]$ chmod 777 dwd_db.sh
### （3）执行脚本导入数据 ###
		[hadoop@node1 bin]$ ./dwd_db.sh 2020-04-28
### （4）查看导入数据 ###
		hive (gmall)>
		select * from gmall.dwd_sku_info where dt='2020-04-28' limit 2;

# DWS层 #


# 用户购买商品明细表（宽表） #
## 创建用户购买商品明细表 ##
		hive (gmall)>
		drop table if exists gmall.dws_sale_detail_daycount;
		create external table gmall.dws_sale_detail_daycount
		(   
		    user_id   string  comment '用户 id',
		    sku_id    string comment '商品 Id',
		    user_gender  string comment '用户性别',
		    user_age string  comment '用户年龄',
		    user_level string comment '用户等级',
		    order_price decimal(10,2) comment '商品价格',
		    sku_name string   comment '商品名称',
		    sku_tm_id string   comment '品牌id',
		    sku_category3_id string comment '商品三级品类id',
		    sku_category2_id string comment '商品二级品类id',
		    sku_category1_id string comment '商品一级品类id',
		    sku_category3_name string comment '商品三级品类名称',
		    sku_category2_name string comment '商品二级品类名称',
		    sku_category1_name string comment '商品一级品类名称',
		    spu_id  string comment '商品 spu',
		    sku_num  int comment '购买个数',
		    order_count string comment '当日下单单数',
		    order_amount string comment '当日下单金额'
		) COMMENT '用户购买商品明细表'
		PARTITIONED BY (`dt` string)
		stored as parquet
		location '/warehouse/gmall/dws/dws_user_sale_detail_daycount/'
		tblproperties ("parquet.compression"="snappy");
## 数据导入 ##
		hive (gmall)>
		with
		tmp_detail as
		(
		    select
		        user_id,
		        sku_id, 
		        sum(sku_num) sku_num,   
		        count(*) order_count, 
		        sum(od.order_price*sku_num) order_amount
		    from gmall.dwd_order_detail od
		    where od.dt='2020-04-28'
		    group by user_id, sku_id
		)  
		insert overwrite table gmall.dws_sale_detail_daycount 		partition(dt='2020-04-28')
		select 
		    tmp_detail.user_id,
		    tmp_detail.sku_id,
		    u.gender,
		    months_between('2020-04-28', u.birthday)/12  age, 
		    u.user_level,
		    price,
		    sku_name,
		    tm_id,
    			category3_id,
    			category2_id,
    			category1_id,
    			category3_name,
    			category2_name,
    			category1_name,
    			spu_id,
    			tmp_detail.sku_num,
    			tmp_detail.order_count,
    			tmp_detail.order_amount 
		from tmp_detail 
		left join gmall.dwd_user_info u on tmp_detail.user_id =u.id 		and u.dt='2020-04-28'
		left join gmall.dwd_sku_info s on tmp_detail.sku_id =s.id and s.dt='2020-04-28'
		;
## 数据导入脚本 ##
### (1）在/home/hadoop/bin目录下创建脚本dws_sale.sh ###
		[hadoop@node1 bin]$ vim dws_sale.sh
在脚本中填写如下内容

		#!/bin/bash

		# 定义变量方便修改
		APP=gmall
		hive=/opt/bigdata/hive/bin/hive
		
		# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
		if [ -n "$1" ] ;then
			do_date=$1
		else 
			do_date=`date -d "-1 day" +%F`  
		fi 
		
		sql="
	
		set hive.exec.dynamic.partition.mode=nonstrict;
		
		with
		tmp_detail as
		(
		    select 
		        user_id,
		        sku_id, 
		        sum(sku_num) sku_num,   
		        count(*) order_count, 
		        sum(od.order_price*sku_num)  order_amount
		    from "$APP".dwd_order_detail od
		    where od.dt='$do_date'
		    group by user_id, sku_id
		)  
		insert overwrite table "$APP".dws_sale_detail_daycount 			partition(dt='$do_date')
		select 
		    tmp_detail.user_id,
		    tmp_detail.sku_id,
		    u.gender,
		    months_between('$do_date', u.birthday)/12  age, 
		    u.user_level,
		    price,
		    sku_name,
		    tm_id,
		    category3_id,
		    category2_id,
		    category1_id,
		    category3_name,
		    category2_name,
		    category1_name,
		    spu_id,
		    tmp_detail.sku_num,
    			tmp_detail.order_count,
    			tmp_detail.order_amount 
		from tmp_detail 
		left join "$APP".dwd_user_info u 
		on tmp_detail.user_id=u.id and u.dt='$do_date'
		left join "$APP".dwd_sku_info s on tmp_detail.sku_id =s.id  and s.dt='$do_date';

		"
		$hive -e "$sql"

### (2）增加脚本执行权限 ###
		[hadoop@node1 bin]$ chmod 777 dws_sale.sh
### (3）执行脚本导入数据 ###
		[hadoop@node1 bin]$ ./dws_sale.sh 2020-04-28
### (4）查看导入数据 ###
		hive (gmall)>
		select * from gmall.dws_sale_detail_daycount where dt='2020-04-28' limit 2;

# ADS层 #
# 用户画像

## 建表和导入数据语句 ##
```
create table ads_sku_top3 as
select * from (
select t.*,row_number() over(PARTITION by t.user_id order by t.total desc) as seq from (
SELECT
 user_id,
 sku_category1_name category_1,
 sum(order_count) total
FROM
 tmp_dws_sale_detail_daycount
GROUP BY
 user_id,
 sku_category1_name
ORDER BY
 user_id,
 total DESC
) t
)t2 where t2.seq <= 3

create table tmp as select user_id, if(user_gender='M',"男","女") as gender, cast(user_age as int) as age from tmp_dws_sale_detail_daycount;

create table tmp2 as select distinct * from tmp;

create table ads_sku3_top3 as  select a.*,b.gender,b.age  from ads_sku_top3 a left join tmp2 b on a.user_id=b.user_id;
```
